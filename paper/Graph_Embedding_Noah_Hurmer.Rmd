---
title: |
  ![](assets/lmu.jpg)
  NLP - Graph Embedding
subtitle: |
  | A Brief Introduction into GE Models utilizing Random Walk
  |
  |
  |
  | Supervisor: M.Sc. Matthias Assenmacher
author: "Noah Hurmer"
date: "tbd"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 3
    fig_width: 7
    highlight: tango
    number_sections: yes
fontsize: 11pt
geometry: margin=2.5cm
header-includes:
- \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
- \usepackage{graphics}
- \usepackage{setspace}\onehalfspacing
- \usepackage{float}
- \setlength{\parskip}{0em}
- \usepackage[font={small,it}, labelfont={bf}]{caption}
- \usepackage{amsmath}
bibliography: references.bib

nocite: '@*'

abstract: \textcolor{red}{TODO}
---
\newpage


```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", out.extra = "")
library(tidyverse)
library(data.table)
library(checkmate)
library(knitr)
library(kableExtra)
```

\newpage

\newcounter{savepage}
\pagenumbering{Roman}

\setcounter{tocdepth}{4}
\tableofcontents

\newpage

\listoftables


\listoffigures


\section*{List of Abbreviations}

```{r abbreviations, echo=FALSE}
data.table(
  Term = c("GE", "DL", "RW"),
  Abbreviation = c("Graph Embedding", "Deep Learning", "Random Walk")) %>%
  arrange(Term) %>%
  kable(booktabs = TRUE, format =  "latex", escape = FALSE) %>%
  kable_styling(latex_options = c("repeat_header", "striped"), full_width = TRUE)
```


\newpage

\setcounter{savepage}{\arabic{page}}

\pagenumbering{arabic}

# Introduction

\textcolor{red}{Dont really know what to put here yet tbh.}

## Orginization

\textcolor{red}{TODO write out}

  - written table of context
  - section 1 describes graphs their types and applications
  - section 2 embedding motivation, types and associated downstream tasks
  - section 3 shows the use of a Random Walk in order to use a DL model to embed with the example of node2vec
  - section 4 tries to tie back into NLP, showing RW principles at work in order to perform cQA related tasks

## Data structure

  Graphs are a type of data structure that consist of so called nodes (or vertices) and edges. Therefore, a graph $G$ is usually noted as $G=(V, E)$. Here, $V$ is the set of nodes and $E$ the set of edges.
  \textcolor{red}{source}  
  
  Nodes typically represent some form of object or entity. This can range from a physical object, component or location to theoretical concepts. The data form of a node is therefore generally not limited to any specific form, and can be anything from text to images, videos, lists and many more.  However, most embedding and downstream tasks only deal with a label per node, as the data it represents is often not necessarily required.  
  
  Edges describe the relation between a pair of nodes, which can equivalently portray physical properties such as proximity and connectivity or other links such as effects, associations or relationships. An apparent example of these abstract or complex concepts a graph can embody is a social media network. In such a graph, people or their representative profiles would be the nodes and friendships, follows, comments, likes, etc. the edges.  
  
## Graph types
  
  These pairwise relations of a graph can also be directed in what is known as a directed or directional Graph, where the given relation only applies from one node to the other, yet not (necessarily) the other way around. The relation is therefore not symmetric and also generally does not need to be reflexive either. For instance in a social media network, the edges may be a 'following' status or something equivalent. This relation between two people is directional.  
  
  In addition, an edge between two nodes can be weighted, where it receives a numerical label. This can represent concepts such as a cost of traversal, distance or capacity. Generally, it gives the relation of the graph a (metric) scale. This type of graph is then called a weighted Graph.
  A simple example of this might be a road network. Here nodes can represent intersections or cities, dependent on the scale and edges connecting roads or highways. The roads can then be attributed with a time cost to use them.
 \textcolor{red}{sources}
  
  Graphs may also be of heterogeneous mode, meaning different types or classes of nodes as well as edges can exist in a single graph. This is useful to embody networks of different interactions or effects from objects to one another. A very basic example of a heterogeneous graph is a simple semantic graph, which represents semantic relationships such as "is a" or "has" and so forth.
  \textcolor{red}{sources, maybe example graph plot}  
  
  To return to the example of social networks however, these are also often heterogeneous. An example we will return to later is a so-called cQA or community Question and Answer forum. These typically have Objects of the classes "Question", "Answer" and "Profile", as well as edges such as "asked", "answered", "follows", "upvoted" and possibly more.
  \textcolor{red}{source}
  

# Embedding

  Embedding refers to the representation of an object by another object. Typically, this representation is a two-dimensional vector.  
  
  \textcolor{red}{Dunno bout this part.}

## Motivation

  Two of the main issues with data in the form of a graph are its inherent structure and the limited applicable mathematics available to deal with that. Additionally, the computational challenge associated with any type of storage or calculation performed on it poses a problem.
  
  The above described composition of a graph is usually stored in a so called adjacency matrix, with the dimensions of $N\times N$, where $N$ is the number of nodes in the graph. The edges are then captured with a binary indicator (or a value for a weighted edge), whether or not two nodes are connected via an edge.
  The sheer size of such matrices can quickly become a problem both in form of (dynamic) storage space but also computational expense. 
  
  Therefore we aim to compress the information of a graph down to lower dimensions and into a form that lets us better apply analysis tools. Usually a two dimensional vector space is selected as the embedding dimension. This projects the graph into an euclidean space and enables the application of a distance metric between objects in that space, enabling downstream tasks that rely on this.  
  
  Moreover, as long as the graph does not change, once a graph has been embedded, multiple tasks and calculations can be performed without the need to embed anew, resulting in a performance gain.  
  
  \textcolor{red}{source}

## Types and Applications

  \textcolor{red}{combine and write better}

  As stated, graph embedding simply lets us efficiently work with the data type of a graph. So applications are just uses, where data is in the form of a graph.

  - uses in social networks
  - visualization
  - Network compression
  - Network Partitioning
  - Node Classification
  - Link Prediction
  - fake news detection 

  
  There are different forms of graph embedding, each with their own uses and specific tasks. These are usually coupled with the structure that is preserved.
  
  Entire Graphs can be embedded into a low dimensional vector space in order to compare different graphs to each other. Similar Graphs are embedded close to each other. This can for example be useful for biologists to compare proteins or predict their functional labels. Here, a complex protein can be a graph which will then be embedded to a single vector.
  
  The most common way of graph embedding is to embed the nodes of a graph. Here, each node is represented as a vector in a low dimensional space, where embeddings of similar or close nodes are located close to each other. This proximity can be defined in different ways. A typical metric for this is a proximity of order $n$, which describes the similarity of the $n$-order neighbourhoods of the respective nodes.
  
  A different approach is to embed the edges between nodes. The so called edge- or knowledge embedding aims to preserve relations between node pairs. This is particularly useful to predict missing or validate existing links in a graph or to predict missing relations between entities.
  
  It is also possible to embed subgraphs or groups (communities) of a graph separately. This is often referred to as hybrid embedding, as it combines aspects of node- and edge embedding to support tasks such as clustering or semantic searches.
  

## Techniques

There are several different approaches used to embed a graph. As mentioned above, different tasks require different information to be preserved about the graph. So mostly, a given model is  designed for a specific task or goal and thus the type of input graph it can accept is often limited. \textcolor{red}{missing something here}
  
Embedding models are generally summarized into the following categories.  

### Matrix Factorization
  
This technique mostly represents the beginnings of GE and has the disadvantage of high computational and storage cost, as a Matrix of pairwise node similarity is constructed, that is then factorized to obtain the embedding of each node.
\textcolor{red}{explain this more}
    
Due to the fact that all node-pairs are considered (unlike other techniques), it can be quite performant, however the cost disadvantages make this largely infeasible for larger graphs.  
    
  
### Deep Learning
  
The use of DL models in GE has recently become more popular, as they usually carry the promise of great efficiency and performance. Simply put, existing (or purpose-built) DL models are applied in order to embed information into vector form. These models typically use Autoencoder or CNN based methods. (@survey)
However, the input to such models can also be paths sampled from a graph. These sampled paths are strings of nodes visited during a sampling strategy. This method is called Random Walk and can be seen as its own subcategory of DL embedding models. (@GOYAL201878)

### Others

  While other methods exists, there are considerably less applications of such and tend to be summarized as "Other Methods" or similar. (@GOYAL201878)
  @survey however, divide them into three main categories: Edge Reconstruction, Graph Kernel based methods and Generative Models.

## Performance Evaluation

  short section on micro and macro F1 scoring

# node2vec
  
  *node2vec* (@node2vec) is an example of such a Random Walk based DL approach to Graph Embedding. It usually represents nodes of a Graph in the feature space and is therefore a Node embedding model. 
  
## Random Walk

  The Random Walk principle is to sample paths from a graph in order to estimate the proximity of a node pair.  

  Similar to *DeepWalk* (@DeepWalk), the *node2vec* algorithm has its origins in Natural Language Processing, as it adapts the idea of *Word2vec* (@Word2vec). In NLP, tasks include to predict the surrounding words in a context window given a specific word. Or put simpler, prediction of context given a word. These context widows, often part-sentences or whole sentences, are input into a DL Model called *skipgram*. From this, a vector representation of a word is gained or a so called word-embedding. \textcolor{red}{todo source for skipgram}  
  
  In *node2vec* and other RW based Node embedding models, artificial "sentence"-equivalents are created by sampling paths or walks within a graph. These paths are our context windows for the *skipgram* model and can be directly input as such.  
  
   Each node is used as a starting point, from which neighbouring nodes are sampled in succession to obtain a path of desired length $l$.
  The probability to reach a node $x$ from a previous node $v$ is therefore given by the number of edges at the node $v$ and is 0 if $x$ and $v$ are not directly connected by an edge.
  More generally the probabilities are given by
  
\begin{equation}
\label{eqn:RWprob}
  P(c_i=x|c_{i-1}=v) = 
  \begin{cases}
    \frac{\pi_{v,x}}{Z},& \text{if } (v, x) \in E\\
    0,&\text{otherwise}
    
  \end{cases}
\end{equation}

where $c_i$ denotes the $i$th node of a RW, $\pi_{v,x}$ the transition probability from node $v$ to node $x$ and $Z$ a nomalizing constant. In the basic RW form, the transition probability is 1 if the graph is unweighted and $Z$ is the number of edges originating from node $v$. (@node2vec)  
  
  In comparison to other strict searching algorithms to create neighbourhoods of nodes, there are clear spatial and time cost advantages of RW, as they can easily be parallelised and depending on the order and length of the RW do not need a lot of memory capacity. In addition, this technique usually does a decent job of capturing the structure of a graph, as the power-law remains. In simple terms, a well connected node will be visited more often. (@DeepWalk)  
  
### RW in node2vec

  The *node2vec* model is a slight modification of a basic Random Walk. Here, two parameters are introduced to modify a RW. These modifiers, typically denoted $p$ and $q$, add a bias to the sampling of nodes during the RW in order to control mostly the locality of the walk. This results in a mixture of BFS and DFS and can therefore represent different types of network neighbourhoods and node equivalences (@node2vec).
  The random walk is of 2\textsuperscript{nd} order, meaning that the previous node visited has an impact on the sampling of the next node of a RW traverse. In this case, the probability of a node $x$ to become the next node in the RW {$..., t, v$}, is biased not only by the weight of the edge ($v$, $x$) ($w_{i,j} = 1; \forall{i, j \in V}$ if the graph is unweighted), but also by the distance to the previous node $d_{t, x}$, where
  
$$
\alpha_{p,q}(t,x) = 
\begin{cases}
    \frac{1}{p},& \text{if } d_{t,x} = 0\\
    1,         &  \text{if } d_{t,x} = 1\\
    \frac{1}{q},& \text{if } d_{t,x} = 2
\end{cases}
$$

  This bias $\alpha$ is then multiplied by the weight of the edge $w_{v,x}$ to obtain $\pi_{v,x}$ of equation \ref{eqn:RWprob}.
  $p$, also called the Return Parameter (@node2vec), defines the probaility of backtracing a step, while $q$, also called the In-Out Parameter (@node2vec), controls the exploratory nature of the walk. Setting $p$ low and $q$ high therefore keeps the walk very local, whilst a high $p$ almost acts like a small self-avoiding-walk (@saw) and a low $q$ ensures that the walk travels further outward from the starting node.
  
  
```{r exGraph, fig.cap = "Random example Graph"}

include_graphics("plots/exGraph.pdf")

```
  
## SkipGram

  \emph{node2vec} like many other RW-based DL embedding methods, utilizes a variation of skipGram as the DL model (@survey).
  SkipGram originates in NLP with word embeddings. The model attempts to predict surrounding words of a certain range in a sentence, given a current word. The current word is used as an input to a log-linear classifier.
  Projecting this method to the task of graph prediction, a RW is to be interpreted as a sentence and a node as a word. SkipGram can then be applied on the sampled paths in order to maximize the probability to oberserve a neighbourhood of a node $v$, based on the feature representation of that node. 
  So, let $N(u)$ be the neighborhood of the node $u$ and $f$ the projecting function to our feature space: $f : V \to \mathbb{R}^d$ , where $d$ denotes the number of dimensions of the desired feature space.
  The objective function results in

\begin{equation}
\label{eqn:objecfun}
  \max_{f} \sum_{u \in V} log(P(N(u)|f(u)))
\end{equation}

To simplify, we assume conditional indepence in a neighbourhood so that the likelihood to observe a neighbourhood can be defined by factorizing over the likelihoods to observe each neighbourhood node. 

$$
log(P(N(u)|f(u))) = \prod_{n_i\in N(u)} P(n_i|f(u))
$$
 Also, we define the conditional likelihood between two nodes as a softmax equation.
 
$$
P(n_i|f(u)) = \frac{exp(f(ni) * f(u))}{\sum_{v\in V} exp(f(v) * f(u))}
$$
  These assumptions transform the objective function \ref{eqn:objecfun} to

$$
\max_{f} \sum_{u \in V} -log \left( \sum_{v \in V} exp(f(v) * f(u)) \right) + \sum_{n_i\in N(u)} f(n_i) * f(u)
$$
  
  Maybe put this somewhere else  
  The node-neighbourhoods in \emph{node2vec} are defined by the Random Walks, so one can observe, how the walk defining parameters $p$ and $q$ directly effect the embedding, as nodes with similar neighbourhoods will end up with similar feature representations.
  
### Negative Sampling

\textcolor{red}{TODO}

  However, the first sum, also called the per-node partition function 
$\sum_{v \in V} exp(f(v) * f(u))$ is too expense to compute directly, so \emph{node2vec} utilizes negative sampling to estimate this function.

## Extensions and Applications

\textcolor{red}{TODO}

  @node2vec have also shown, how when adapting the *node2vec* model to represent a pair of nodes (an edge) in the feature space, it then also lends itself to tasks such as link prediction.
  
  LSTM node2vec combination
  
  Walklets
  
  RW with skips

# Outlook into other RW models

\textcolor{red}{TODO}

  - elaborate on cQA
  - introduce LSTM
  - explain tasks associated
  


\section{References}
