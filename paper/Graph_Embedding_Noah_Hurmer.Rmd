---
title: |
  ![](assets/lmu.jpg)
  NLP - Graph Embedding
subtitle: |
  | A Brief Introduction into GE Models utilizing Random Walk
  |
  |
  |
  | Supervisor: M.Sc. Matthias Assenmacher
author: "Noah Hurmer"
date: "tbd"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 3
    fig_width: 7
    highlight: tango
    number_sections: yes
fontsize: 11pt
geometry: margin=2.5cm
header-includes:
- \usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
- \usepackage{graphics}
- \usepackage{setspace}\onehalfspacing
- \usepackage{float}
- \setlength{\parskip}{0em}
- \usepackage[font={small,it}, labelfont={bf}]{caption}
- \usepackage{amsmath}
bibliography: references.bib

nocite: '@*'

abstract: TODO
---
\newpage


```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", out.extra = "")
library(tidyverse)
library(data.table)
library(checkmate)
library(knitr)
library(kableExtra)
```

\newpage

\newcounter{savepage}
\pagenumbering{Roman}

\setcounter{tocdepth}{4}
\tableofcontents

\newpage

\listoftables


\listoffigures


\section*{List of Abbreviations}

```{r abbreviations, echo=FALSE}
data.table(
  Term = c("GE", "DL", "RW"),
  Abbreviation = c("Graph Embedding", "Deep Learning", "Random Walk")) %>%
  arrange(Term) %>%
  kable(booktabs = TRUE, format =  "latex", escape = FALSE) %>%
  kable_styling(latex_options = c("repeat_header", "striped"), full_width = TRUE)
```


\newpage

\setcounter{savepage}{\arabic{page}}

\pagenumbering{arabic}



# Introduction


## What is a graph

  Graphs are a type of data structure that consist of so called nodes (or vertices) and edges. Often times, the nodes describe entities and the edges the pairwise relation to one another. Therefore, a graph $G$ is usually noted as $G=(V, E)$. Here, $V$ is the set of nodes and $E$ the set of edges.
  
  In essence, this can be thought of as a network of entities. One of the simpler realizations to imagine is perhaps a social media platform. Where a person (or their profile) is represented as a node and an edge indicates that two people are befriended on the platform.
  As apparent in the above mentioned example, graphs can have edges that embody abstract concepts, that do not intuitively fit into any conventional coordinate system, as they cannot be mapped into an euclidean space.
  That being said, one can also imagine more conventional observed data of a large feature space as a graph, where the edges are weighted and represent a similarity between objects.

## Graph types

  The type of entity that a node represents can be in any form of data, be that words or text, images



  
perhaps omit some examples as to not overcrowd or overcomplicate this passage.

## Graph types

  - What is a Graph? 
  - Nodes and edges
  - Nodes are the entities
  - Edges are the relations between nodes
  - nodes can consist of all sort of Data, images, text, etc even mixed
  - nodes can have additional attributes
  
### Graph modes

  - homogeneous, heterogeneous
  - graphs can be directional, weighted, semantik, knowledge based
  

## Motivation

  Two of the main issues with data in the form of a graph are its inherent structure and the limited applicable mathematics available to deal that, and the computational challenge associated with any type of storage or calculation performed on it.
  
  The above described composition of a graph is usually stored in a so called adjacency matrix, with the dimensions of $N\times N$, where $N$ is the number of nodes in the graph. The edges are then captured with a binary indicator (or a value for a weighted edge), whether or not two nodes are connected via an edge.
  Not only does this become a problem if the relationship the edge represents becomes more complex, but the sheer size of such matrices can quickly become a problem both in form of (dynamic) storage space but also computational expense. 
  
  Therefore we aim to compress the information of a graph down to lower dimensions and into a form that lets us better apply analysis tools. Usually a vector space is selected as the embedding dimension.   
  
  
## Applications

  As stated, graph embedding simply lets us efficiently work with the data type of a graph. So applications are just uses, where data is in the form of a graph.

  - uses in social networks
  - visualization
  - Network compression
  - Network Partitioning
  - Node Classification
  - Link Prediction
  - fake news detection 


## Embedding types
  
  There are different forms of graph embedding, each with their own uses and specific tasks. These are usually coupled with the structure that is preserved.
  
  Entire Graphs can be embedded into a low dimensional vector space in order to compare different graphs to each other. Similar Graphs are embedded close to each other. This can for example be useful for biologists to compare proteins or predict their functional labels. Here, a complex protein can be a graph which will then be embedded to a single vector.
  
  The most common way of graph embedding is to embed the nodes of a graph. Here, each node is represented as a vector in a low dimensional space, where embeddings of similar or close nodes are located close to each other. This proximity can be defined in different ways. A typical metric for this is a proximity of order $n$, which describes the similarity of the $n$-order neighbourhoods of the respective nodes.
  
  A different approach is to embed the edges between nodes. The so called edge- or knowledge embedding aims to preserve relations between node pairs. This is particularly useful to predict missing or validate existing links in a graph or to predict missing relations between entities.
  
  It is also possible to embed subgraphs or groups (communities) of a graph separately. This is often referred to as hybrid embedding, as it combines aspects of node- and edge embedding to support tasks such as clustering or semantic searches.
  

# Problem and Dataset

  introduce the Problem and the associated Dataset on which the algorithms below should be run on.


# Techniques

  There are several techniques that can be used to embed a graph, however these are mostly coupled with the task or goal, the type of input graph and each have their own disadvantages.

  - Matrix Factorization
  
    This technique mostly represents the beginnings of GE and has the disadvantage of high computational and storage cost, as a Matrix of pairwise node similarity is constructed, that is then factorized to obtain the embedding of each node. Due to the fact that all node-pairs are considered (unlike other techniques), it can be quite performant, however the cost disadvantages make this largely infeasible for larger graphs.
    
  
  - Deep Learning
  
    Recently more popular is the use of DL models. Simply put, existing (or purpose-built) DL models are applied on the input
    
  - Edge Reconstruction
  
  - Graph Kernel
  
  - Generative Model

  Short Paragraph explaining how one Approach to (node)Graph embedding utilizes Deep Learning methods. List the other approaches not using DL with a brief explanation, and especially mention how the computational advantage of the Random Walk approach can be preferable to MF.
  
## Random Walk
  
  One category of DL based graph embedding utilizes the Random Walk principle to sample paths from a graph in order to estimate the proximity of a node pair. Each node is used as a starting point, from which neighbouring nodes are sampled in succession to obtain a path of desired length $l$.
  The probability to reach a node $x$ from a previous node $v$ is therefore given by the number of edges at the node $v$ and is 0 if $x$ and $v$ are not directly connected by an edge.
  More generally the probabilities are given by
  
\begin{equation}
\label{eqn:RWprob}
  P(c_i=x|c_{i-1}=v) = 
  \begin{cases}
    \frac{\pi_{v,x}}{Z},& \text{if } (v, x) \in E\\
    0,&\text{otherwise}
    
  \end{cases}
\end{equation}

where $c_i$ denotes the $i$th node of a RW, $\pi_{v,x}$ the transition probability from node $v$ to node $x$ and $Z$ a nomalizing constant. In the basic RW form, the transition probability is 1 if the graph is unweighted and $Z$ is the number of edges originating from node $v$. (@node2vec)
  
  In comparison to other strict searching algorithms to create neighbourhoods of nodes, there are clear spatial and time cost advantages of RW, as they can easily be parallelized and depending on the order and length of the RW do not need a lot of memmory capacity. In addition, this technique usually does a decent job of capturing the structure of a graph, as the power-law remains. In simple terms, a well connected node will be visited more often. (@DeepWalk)


### Random Walk in node2vec

  Here, two parameters are introduced to modify a RW. These modifiers, typically denoted $p$ and $q$, add a bias to the sampling of nodes during the RW in order to control mostly the locality of the walk. This results in a mixture of BFS and DFS and can therefore represent different types of network neighbourhoods and node equivalences (@node2vec).
  The random walk is of 2\textsuperscript{nd} order, meaning that the previous node visited has an impact on the sampling of the next node of a RW traverse. In this case, the probability of a node $x$ to become the next node in the RW {$..., t, v$}, is biased not only by the weight of the edge ($v$, $x$) ($w_{i,j} = 1; \forall{i, j \in V}$ if the graph is unweighted), but also by the distance to the previous node $d_{t, x}$, where
  
$$
\alpha_{p,q}(t,x) = 
\begin{cases}
    \frac{1}{p},& \text{if } d_{t,x} = 0\\
    1,         &  \text{if } d_{t,x} = 1\\
    \frac{1}{q},& \text{if } d_{t,x} = 2
\end{cases}
$$
  This bias $\alpha$ is then multiplied by the weight of the edge $w_{v,x}$ to obtain $\pi_{v,x}$ of equation \ref{eqn:RWprob}.
  $p$, also called the Return Parameter (@node2vec), defines the probaility of backtracing a step, while $q$, also called the In-Out Parameter (@node2vec), controls the exploratory nature of the walk. Setting $p$ low and $q$ high therefore keeps the walk very local, whilst a high $p$ almost acts like a small self-avoiding-walk (@saw) and a low $q$ ensures that the walk travels further outward from the starting node.
  
  
```{r exGraph, fig.cap = "Random example Graph"}

include_graphics("plots/exGraph.pdf")

```
  

## SkipGram

  \emph{node2vec} like many other RW-based DL embedding methods, utilizes a variation of skipGram as the DL model (@survey).
  SkipGram originates in NLP with word embeddings. The model attempts to predict surrounding words of a certain range in a sentence, given a current word. The current word is used as an input to a log-linear classifier.
  Projecting this method to the task of graph prediction, a RW is to be interpreted as a sentence and a node as a word. SkipGram can then be applied on the sampled paths in order to maximize the probability to oberserve a neighbourhood of a node $v$, based on the feature representation of that node. 
  So, let $N(u)$ be the neighborhood of the node $u$ and $f$ the projecting function to our feature space: $f : V \to \mathbb{R}^d$ , where $d$ denotes the number of dimensions of the desired feature space.
  The objective function results in

\begin{equation}
\label{eqn:objecfun}
  \max_{f} \sum_{u \in V} log(P(N(u)|f(u)))
\end{equation}

To simplify, we assume conditional indepence in a neighbourhood so that the likelihood to observe a neighbourhood can be defined by factorizing over the likelihoods to observe each neighbourhood node. 

$$
log(P(N(u)|f(u))) = \prod_{n_i\in N(u)} P(n_i|f(u))
$$
 Also, we define the conditional likelihood between two nodes as a softmax equation.
 
$$
P(n_i|f(u)) = \frac{exp(f(ni) * f(u))}{\sum_{v\in V} exp(f(v) * f(u))}
$$
  These assumptions transform the objective function \ref{eqn:objecfun} to

$$
\max_{f} \sum_{u \in V} -log \left( \sum_{v \in V} exp(f(v) * f(u)) \right) + \sum_{n_i\in N(u)} f(n_i) * f(u)
$$
  
  Maybe put this somewhere else  
  The node-neighbourhoods in \emph{node2vec} are defined by the Random Walks, so one can observe, how the walk defining parameters $p$ and $q$ directly effect the embedding, as nodes with similar neighbourhoods will end up with similar feature representations.
  
### Negative Sampling

  However, the first sum, also called the per-node partition function 
$\sum_{v \in V} exp(f(v) * f(u))$ is to expense to compute directly, so \emph{node2vec} utilizes negative sampling to estimate this function.
    
    
## (LSTM)

  Optional Section considered as an excursion to another Random Walk method using a different Model and heterogeneous Graphs as Inputs. Perhaps using HSNL as the algorithm example.



# Performance Evaluation

  Discuss how to evaluate Graph Embedding methods and algorithms and then evaluate the above used ones.



\section{References}
