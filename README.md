# Graph Embedding
## LMU NLP - BA - Seminar

Date of presentation: tbd  
Institut: Statistik, LMU Muenchen   
Lecture: BA-Seminar NLP  
Project advisor: M.Sc. Matthias Assenmacher  
Author: Noah Hurmer  


### Abstract

Graphs appear in all types of contexts and fields, as they embody the structure of networks, such as online social networks, word co-occurrences, biological structures and interactions or commerce and advertising structures to name a few. To enable performant and efficient handling of such a data structure, information of a graph is embedded into a defined feature space. This then lends itself to downstream tasks such as labelling, classification, clustering, link prediction and semantic searches. NLP and its associated task of word embedding have birthed embedding models such as Word2vec (Mikolov, Chen, et al. (2013)). This architecture was then adapted for general graph embedding use with the help of the Random Walk concept. The example of this given here will be node2vec (Grover and Leskovec (2016)). In this report, we start from the beginning, as we introduce graphs, explain their composition and structure, continue on to known embedding types and techniques and explore node2vec in detail.
